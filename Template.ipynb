{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Radio Lightcurves\n",
    "\n",
    "\n",
    "## Key steps\n",
    "- Plot the radio light curve\n",
    "- Determine its spectral index\n",
    "- Scale the data based on its spectral index\n",
    "- Fit the data with a power law\n",
    "- Fit the data with a broken power law with a smooth turnover\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path = sys.path[:-2]\n",
    "path_temp = sys.path\n",
    "path = [sys.path[-1]]\n",
    "path.extend(sys.path[:-1])\n",
    "sys.path = path\n",
    "import numpy as np\n",
    "from astropy.io import ascii\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "from scipy.optimize import least_squares, curve_fit\n",
    "from scipy.stats import f\n",
    "import emcee\n",
    "import corner\n",
    "import os\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the data\n",
    "get the data out from papers into excel sheets, or any other forms that can be easily read by python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the space to include data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Make a plot of the radio lightcurve\n",
    "We will now plot the flux density as a function of time (the lightcurve). We use different coloured markers to denote the observing frequency, and different marker styles to denote different telescopes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Determining the spectral index\n",
    "As with many things in astrophysics, the emission in the radio regime is _non-thermal_ in origin (unlike the early emission in the optical/UV/infrared, which is a thermal blackbody).  What that means is that generally has a power-law spectrum:\n",
    "$$ S_\\nu(\\nu) \\propto \\nu^\\alpha$$\n",
    "Strictly this only works for data observed at _exactly_ the same time, but real observations don't work that way.  A single telescope can (usually) only observe at a single frequency, and different telescopes are separated in time by schedules, longitude, etc.  So we need to be a bit more generous about how we define simultaneous. Luckily, the light-curve is mostly evolving pretty slowly, so this isn't necessarily a problem.\n",
    "\n",
    "Here are two functions to take a subset of the data that was observed roughly simultaneously and calculate the spectral index $\\alpha$ and its uncertainty using multi-band observation at 162 days post-merger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_power_law(freq,S0,alpha):\n",
    "    S = S0 * (freq) ** alpha\n",
    "    return S\n",
    "\n",
    "def alpha_calc(data):    \n",
    "    #Get lightcurve values\n",
    "    freqs = data['frequency']\n",
    "    flux = data['flux']\n",
    "    flux_errs = data['rms']\n",
    "    \n",
    "    #Use the scipy curve_fit algorithm to calculate the best fit value\n",
    "    popt, pcov = curve_fit(calc_power_law, freqs, flux ,sigma=flux_errs, p0=(50,-0.61),absolute_sigma=True)\n",
    "    \n",
    "    alpha = popt[1] #Best-fit spectral index\n",
    "    alpha_err = np.sqrt(np.diag(pcov))[1] #Uncertainty in alpha\n",
    "    \n",
    "    return alpha, alpha_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the data at the ~162 day epoch and print the spectral index + uncertainty\n",
    "\n",
    "sel_data = data[data['delta_t'] == 162.89]\n",
    "\n",
    "alpha, alpha_err = alpha_calc(sel_data)\n",
    "\n",
    "print(\"alpha = %.1f+/-%.1f\"%(alpha, alpha_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Scaling the data based on the spectral Index\n",
    "Based on the $\\alpha$ you calculated above, what happens if you assume that _all_ of the observations can be fit using the same frequency power-law?  i.e., if $\\alpha$ were the same at all times?  If this is the case then scaling all of the data to a single frequency makes it easier to understand the temporal properties of the lightcurve as a function of only 1 variable, not 2.\n",
    "\n",
    "Write a function to take the observed data and scale it to a specific frequency based on an estimated spectral index. \n",
    "\n",
    "Don't forget to include uncertainties! You should add two columns to your data table called \"scaled_flux\" and \"scaled_rms\".\n",
    "\n",
    "Questions:\n",
    "1. Can you think of reasons why the spectral index should stay the same?  Why it should change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data, alpha, alpha_err, ref_freq=3.0):\n",
    "    #calculate a scaling factor for the flux density and uncertainty\n",
    "    f_scale = ######\n",
    "    rms_scale = ######\n",
    "    \n",
    "    #scale the flux and uncertainty - don't forget to add errors in quadrature\n",
    "    scaled_flux = ######\n",
    "    scaled_rms = ######\n",
    "    \n",
    "    #Add two new columns to the data\n",
    "    data['scaled_flux'] = scaled_flux\n",
    "    data['scaled_rms'] = scaled_rms\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data to 3 GHz based on your estimated spectral index and associated uncertainty, then plot the scaled lightcurve by passing `scaled=True` to your `make_plot` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scale_data(data, alpha, alpha_err)\n",
    "make_plot(data, scaled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Fitting the data\n",
    "We now want to characterise the radio lightcurve. You should be able to see that it initially rises according to a power law, peaks somewhere between 100 and 200 days post-merger and then declines according to a different power law.\n",
    "\n",
    "Create a new table called tdata that we will use to determine the properties of the lightcurve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit this data with a \"smoothed broken power law\", which combines two power laws with a smoothing parameter around the break point. One functional form of this is given by\n",
    "\n",
    "$S_\\nu(t) = S_{\\rm \\nu,peak} \\left[ \\left(\\dfrac{t}{t_{\\rm peak}}\\right)^{-s\\delta_1} + \\left(\\dfrac{t}{t_{\\rm peak}}\\right)^{-s\\delta_2}\\right]^{-1/s}$\n",
    "\n",
    "Here the spectral index is still $\\alpha$, but we've also introduced _temporal_ power-law indices $\\delta_1$ (before the break) and $\\delta_2$ (after the break).  $S_{\\rm \\nu,peak}$ is the flux density at peak, and $s$ controls the smoothness of the break.\n",
    "\n",
    "Write a function smooth_broken_power_law() that outputs a smoothed broken power law that also scales based on frequency and spectral index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_broken_power_law(t, nu, S_peak, t_peak, delta_1, delta_2, alpha, log_s, nu0=3.0):    \n",
    "    S = ##### write your own code\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Markov Chain Monte Carlo\n",
    "\n",
    "We now want to fit a smoothed broken power law to our data. In our paper we do this via a parameter grid-search to minimise the goodness-of-fit parameter $\\chi^2$, i.e., compute $\\chi^2(S_{\\nu,\\rm peak},t_{\\rm peak},\\delta_1,\\delta_2,\\alpha,s)$ for a 6-dimension parameter grid. However, grid searches are slow and innefficient.  It's better to concentrate your effort in the part of the fit where the data \"prefer\" to go.  We can do this using a slightly more complicated statistical technique\n",
    "\n",
    "Here we will perform an Markov Chain Monte Carlo (MCMC) fit using the [`emcee`](http://dfm.io/emcee/current/) package, to determine lightcurve parameters and the spectral index of the source. First you will need to write 3 functions that define your Probability, Prior and Likelihood.\n",
    "\n",
    "We will use a uniform prior with $\\delta_1>0$ (since we require the lightcurve to initially rise), $0<t_{\\rm peak}<300$ (since our data only covers the period up to 200 days) and $s<100$. The parameters will be passed to the function via a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprior(theta):\n",
    "    S_peak, t_peak, delta_1, delta_2, alpha, log_s = theta\n",
    "    \n",
    "    #If the prior conditions are met, return 0.0\n",
    "    if #Prior conditions:\n",
    "        return 0.0\n",
    "    \n",
    "    #Otherwise return -infinity\n",
    "    else:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write a likelihood function that takes the lightcurve parameters inside the tuple `theta`, along with the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnlike(theta, t, nu, S, S_err):\n",
    "    S_peak, t_peak, delta_1, delta_2, alpha, log_s = theta\n",
    "    \n",
    "    model = ####\n",
    "    inv_sigma2 = ####\n",
    "    \n",
    "    lnlike_val = ####\n",
    "    return lnlike_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use a function to calculate the marginal probability using the `lnlike()` and `lnprior()` functions from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(theta, t, nu, S, S_err):\n",
    "    lp = lnprior(theta)\n",
    "\n",
    "    if ####:\n",
    "        return -np.inf\n",
    "    # otherwise return posterial distribution\n",
    "    return lp + lnlike(theta, t, nu, S, S_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit the data using the `emcee` package. The function `get_starting_pos()` provided below will set up an array of walker starting positions for given lightcurve parameters. Examine the lightcurve and estimate some reasonable values for these parameters and add them to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starting_pos(nwalkers, ndim=6):\n",
    "    # set initial position for all the parameters to fit\n",
    "    S_peak =\n",
    "    t_peak = \n",
    "    delta_1 = \n",
    "    delta_2 = \n",
    "    alpha = \n",
    "    log_s = \n",
    "    \n",
    "    pos = \n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function called `run_mcmc()` that will load the observed data, take the starting position and then run the emcee Ensemple Sampler. Use a small number of iterations and walkers initially (100/20) to see how long the code takes to run on your laptop. Then increase both parameters to a larger number so that the algorithm takes ~90 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcmc(data, niters=, nthreads=1, nwalkers=, ndim=):\n",
    "    \n",
    "    \n",
    "    pos = get_starting_pos(nwalkers, ndim=ndim)\n",
    "    \n",
    "    sampler = emcee.EnsembleSampler()\n",
    "    \n",
    "    start = timer()\n",
    "    sampler.run_mcmc()\n",
    "    end = timer()\n",
    "    \n",
    "    print(\"Computation time: %f s\"%(end-start))\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Making Plots and Evaluate Goodness of Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to inspect our chain to see if our algorithm has converged to a reasonable solution. We extract the chain from the sampler and then write a function to make a figure showing how each walker moves around the parameter space. The figure has 6 subplots (1 for each dimension), iteration number on the x-axis and parameter value on the y-axis.\n",
    "\n",
    "MCMC algorithms typically use a burn-in phase, where the sampler is moving towards the optimum solution and not yet accurately sampling the parameter space. Add a parameter chain_cut to your function that plots a vertical line at the end of the burn-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = run_mcmc(tdata)\n",
    "chain = sampler.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chain_plot(chain, chain_cut):\n",
    "    niters = chain.shape[1]\n",
    "    ndim = chain.shape[2]\n",
    "\n",
    "    fig, axes = plt.subplots(ndim,1,sharex=True)\n",
    "    fig.set_size_inches(7, 20)\n",
    "    \n",
    "    param_names = ['$S_{{\\\\nu,\\\\rm peak}, 3\\.{\\\\rm GHz}}$', '$t_{{\\\\rm peak}}$','$\\\\delta_1$','$\\\\delta_2$', '$\\\\alpha$', '$\\\\log_{10}(s)$']\n",
    "\n",
    "    for i, (ax,param_name) in enumerate(zip(axes,param_names)):\n",
    "        #plot the chain for the given parameter\n",
    "        ax.plot(chain[:,:,i].T,linestyle='-',color='k',alpha=0.3)        \n",
    "        \n",
    "        ax.set_ylabel(param_name)\n",
    "        ax.set_xlim(0,niters)\n",
    "        \n",
    "        ax.axvline(chain_cut,c='r',linestyle='--')\n",
    "\n",
    "chain_cut = ######\n",
    "\n",
    "make_chain_plot(chain, chain_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that our algorithm is converging, and we know how long the burn-in takes we can begin to estimate parameters. The function below will make a **corner** plot from the good part of your chain (using the `corner` package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_chain = chain[:, chain_cut:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corner_plot(good_chain, savefile='corner.png'):\n",
    "    param_names = ['$S_{{\\\\nu,\\\\rm peak}, 3\\.{\\\\rm GHz}}$', '$t_{{\\\\rm peak}}$','$\\\\delta_1$','$\\\\delta_2$', '$\\\\alpha$', '$\\\\log_{10}(s)$']\n",
    "    ndim = good_chain.shape[2]\n",
    "    \n",
    "    fig = corner.corner(good_chain.reshape((-1, ndim)), labels=param_names, quantiles=[0.16, 0.5, 0.84], show_titles=True)\n",
    "    plt.savefig(savefile)\n",
    "\n",
    "make_corner_plot(good_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will then extract the median and uncertainty (1 standard deviation) from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(chain):\n",
    "    ndim = chain.shape[2]\n",
    "    \n",
    "    chain = chain.reshape((-1, ndim))\n",
    "    vals = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(chain, [16, 50, 84],axis=0)))\n",
    "    \n",
    "    param_names = ['S_peak', 't_peak', 'delta_1', 'delta_2', 'alpha', 'log_s']\n",
    "    \n",
    "    param_dict = dict(zip(param_names,vals))\n",
    "    \n",
    "    return param_dict\n",
    "    \n",
    "    \n",
    "best_params = get_best_params(good_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function, `calc_chi2()`, that will calculate the $\\chi^2$ for the fit. We will use this later to compare different lightcurve models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_chi2(best_params, param_names, model, data, nu0=3.0):\n",
    "    args = []\n",
    "    for param in param_names:\n",
    "        val = \n",
    "        args.append(val)\n",
    "\n",
    "    best_fit = model(data['delta_t'], nu0, *args)\n",
    "    \n",
    "    chi2 = ######\n",
    "    \n",
    "    return chi2\n",
    "\n",
    "param_names = ['F_peak', 't_peak', 'delta_1', 'delta_2', 'alpha', 'log_s']\n",
    "\n",
    "chi2_best = calc_chi2(best_params, param_names, smooth_broken_power_law, tdata)\n",
    "print(chi2_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot our best fit on top of the observational data.\n",
    "\n",
    "Fill in the function `plot_model()` that takes in a function that calculates the model fit (in this case, our `smooth_broken_power_law` function), the best parameters, an array of values to plot the model for and a matplotlib axis to plot it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model, params, tvals, ax):\n",
    "    best_fit = ######\n",
    "    \n",
    "    ax.plot(tvals,best_fit,marker='',linestyle='-',c='k',linewidth=1.5,zorder=0)\n",
    "    \n",
    "    return\n",
    "\n",
    "plotting_data = scale_data(tdata, best_params['alpha'][0], np.max(best_params['alpha'][1:]))    \n",
    "    \n",
    "make_plot(plotting_data, scaled=True, model=smooth_broken_power_law, params=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(plotting_data, scaled=True, model=smooth_broken_power_law, params=full_args, plot_models=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [F-test](https://en.wikipedia.org/wiki/F-test) is a generalised test that can be used to compare statistical models. In particular, it is useful when comparing two models where one is a restricted form of the other. Write a function calculate_ftest that calculates the F statistic for our two fits and then calculates the corresponding p-value. Hint: We have already imported the [scipy.stats F-distribution model](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f.html), and we can access the cumulative distribution function using f.cdf()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ftest(chi2_t, p_t, chi2_nt, p_nt, n):\n",
    "    F = ######\n",
    "    \n",
    "    pval = f.cdf(F, p_nt, p_t)\n",
    "    \n",
    "    return 1-pval\n",
    "\n",
    "n = ######\n",
    "p_t = ######\n",
    "p_nt = ######\n",
    "\n",
    "calculate_ftest(chi2_best, p_t, chi2_nt, p_nt, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is preferred? With what confidence can we say that we prefer one model over the other?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
